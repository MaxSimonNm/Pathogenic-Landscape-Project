{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Anno Merged File Batch Processor\n",
    "\n",
    "Created on Mon Dec 5 11:37:53 2022\n",
    "\n",
    "@author: nilesh@4basecare.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# To ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For converting csv file to tsv file\n",
    "# Checking indir, if the folder exists and contains \"_merged_output.csv\"\n",
    "folder_path = r\"E:\\filtered_mams_clinical\"\n",
    "\n",
    "def folder_contains_csv(folder_path):\n",
    "    if not os.path.exists(folder_path) or not os.path.isdir(folder_path):\n",
    "        print(f\"The provided path '{folder_path}' is not a valid folder.\")\n",
    "        return False\n",
    "\n",
    "    # List all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Check if any of the files have a \".csv\" extension\n",
    "    for file in files:\n",
    "        if file.lower().endswith('_filtered.csv'):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# Specify the input directory containing CSV files and the output directory for TSV files\n",
    "input_directory = folder_path\n",
    "output_directory = r\"E:\\filtered_mams_clinical\\tsv\"\n",
    "\n",
    "# Ensure the output directory exists, create it if necessary\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Get a list of CSV files in the input directory\n",
    "tsv_files = [filename for filename in os.listdir(input_directory) if filename.endswith('.csv')]\n",
    "\n",
    "# Initialize the progress bar\n",
    "with tqdm(total=len(tsv_files), desc=\"Converting\") as pbar:\n",
    "    # Iterate through all CSV files in the input directory\n",
    "    for filename in tsv_files:\n",
    "        # Construct the full paths for input and output files\n",
    "        input_csv_file = os.path.join(input_directory, filename)\n",
    "        output_tsv_file = os.path.join(output_directory, filename[:-4] + '.tsv')\n",
    "\n",
    "        # Open the CSV file for reading and the TSV file for writing\n",
    "        with open(input_csv_file, 'r') as csv_file, open(output_tsv_file, 'w', newline='') as tsv_file:\n",
    "            # Create a CSV reader and TSV writer\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "\n",
    "            # Iterate through each row in the CSV file and write it to the TSV file\n",
    "            for row in csv_reader:\n",
    "                tsv_writer.writerow(row)\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# Finished converting all CSV files to TSV files in the directory\n",
    "print(\"Conversion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O and Chunk Settings\n",
    "\n",
    "## Folder containing *TSV* MultiAnno Merged Output of Sample's VCF\n",
    "dirpath = r\"E:\\filtered_mams_clinical\\tsv\"\n",
    "\n",
    "## Folder Path for Saving the Chunks\n",
    "savepath = r\"E:\\filtered_mams_clinical_combined\"\n",
    "\n",
    "## Sample List\n",
    "sample_list = r\"H:\\My Drive\\Pathogenic_Landscape\\data\\absolute\\Absolute_clinical.txt\"\n",
    "\n",
    "# Path for Genes file for Gene based filtration\n",
    "# genes = pd.read_csv(\"/home/bioinfo/Nilesh/HRRdb_Samples/Scripts/HRR_genes.txt\", sep='/t')\n",
    "#genes = [\"MSH2\",\"MSH3\",\"MSH5\",\"MSH6\",\"MLH1\",\"MLH2\",\"MLH3\",\"MLH4\"]\n",
    "#genes_series = pd.Series(genes, name=\"MMR Genes\")\n",
    "\n",
    "# Set Number of files per chunk.\n",
    "## If total files is 5, and chunk size is 3,\n",
    "## then 2 folders will be made,\n",
    "## 1st folder will have 3 files, 2nd one will have 2 files\n",
    "chunk_size = 700\n",
    "\n",
    "# Sample File listing\n",
    "tsv_files = [filename for filename in os.listdir(dirpath) if filename.endswith('_filtered.tsv')]\n",
    "\n",
    "# Chunk List creation based on  Chunk Size\n",
    "chunked_list = [tsv_files[i:i+chunk_size] for i in range(0, len(tsv_files), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=dict(enumerate(chunked_list))  #Nested List converted to Dictionary\n",
    "\n",
    "dt_list = [[k,v] for k, values in dt.items() for v in values]\n",
    "\n",
    "# dt_list = []                     #For Loop Expansion of above List Comprehension for Understanding\n",
    "# for keys, values in dt.items():  #for making file list per chunk used later for copying\n",
    "#   for value in values:\n",
    "#       dt_list.append([keys, value])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_index, file_name in dt_list:\n",
    "    source_path = f\"{dirpath}\\\\{file_name}\"\n",
    "    destination_path = f\"{savepath}\\\\Folder{folder_index}\"\n",
    "\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "    # Copy the file\n",
    "    shutil.copy(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/2 [2:06:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m     samples_list\u001b[38;5;241m.\u001b[39mappend(file_df)  \u001b[38;5;66;03m# Append each DataFrame to the list\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     samples \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(samples_list, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Concatenate the list of DataFrames into one\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     \u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_df.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\miniconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\miniconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\miniconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32me:\\miniconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\miniconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\miniconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\miniconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:324\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(res\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[0;32m    323\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[1;32m--> 324\u001b[0m \u001b[43mlibwriters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mwriters.pyx:73\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Standard Filtration\n",
    "for keys in tqdm(dt, desc=\"Processing folders\"):\n",
    "    folder_path = f\"{savepath}\\\\Folder{keys}\"\n",
    "    print(\"####\")\n",
    "\n",
    "    os.chdir(folder_path)\n",
    "    key_files = sorted(Path('./').glob('*.tsv'))\n",
    "    #print(key_files)\n",
    "\n",
    "\n",
    "    samples_list = []  # Create a list to store DataFrames\n",
    "\n",
    "    for file in tqdm(key_files, desc=f\"Processing files in Folder{keys}\", leave=False):\n",
    "        samp_name = file.name.replace(\"_filtered.tsv\", \"\")\n",
    "        sel_cols = ['CHROM_x', 'POS_x', 'End_x', 'REF_x', 'ALT_x', 'Ref.Gene', 'Func.ensGene', 'ExonicFunc.ensGene', 'AAChange.ensGene', 'Interpro_domain', 'avsnp150', 'CLNDN', 'CLNDISDB', 'clinvar: Clinvar ', 'InterVar_automated', 'intervar_inhouse', ' CancerVar: CancerVar and Evidence ', 'OMIM', 'Pathway', 'Therap_list', 'Diag_list', 'Prog_list', 'esp6500siv2_all', 'ExAC_ALL', 'ExAC_SAS', 'AF', 'AF_sas', '1000g2015aug_all', '1000g2015aug_SAS', f'{samp_name}:AF', f'{samp_name}:DP', 'Mutant_allelic_burden_%', 'ensemble_value', 'Ref_Depth', 'Mutant_Depth', 'FILTER']  # f'{samp_name}:GT'\n",
    "        file_df = pd.read_csv(file, sep='\\t', usecols= sel_cols, low_memory=False)\n",
    "        file_df['SID'] = samp_name\n",
    "        file_df['Chr_SERA'] = file_df['CHROM_x'].astype(str) + ' | ' + file_df['POS_x'].astype(str) + ' | ' + file_df['End_x'].astype(str) + ' | ' + file_df['REF_x'].astype(str) + ' | ' + file_df['ALT_x'].astype(str)\n",
    "\n",
    "\n",
    "        # Dynamic column name based on the filename\n",
    "        dyn_AF = f'{samp_name}:AF'\n",
    "        dyn_DP = f'{samp_name}:DP'\n",
    "        dyn_GT = f'{samp_name}:GT'\n",
    "\n",
    "        # Create a new column named 'samp_AF' with values from the dynamic column\n",
    "        file_df['samp_AF'] = file_df[dyn_AF]\n",
    "        file_df['samp_DP'] = file_df[dyn_DP]\n",
    "        #file_df['samp_GT'] = file_df[dyn_GT]\n",
    "\n",
    "\n",
    "        # List of columns to be removed\n",
    "        dynamic_columns_to_remove = [dyn_AF, dyn_DP] #dyn_GT\n",
    "        static_columns_to_remove = ['CHROM_x', 'POS_x', 'End_x', 'REF_x', 'ALT_x']\n",
    "\n",
    "\n",
    "        # Drop the specified columns\n",
    "        file_df.drop(columns= dynamic_columns_to_remove, inplace=True)\n",
    "        file_df.drop(columns= static_columns_to_remove, inplace=True)\n",
    "\n",
    "\n",
    "        # Specify the desired order of columns\n",
    "        desired_columns_order = ['SID', 'Chr_SERA'] + [col for col in file_df.columns if col not in ['SID', 'Chr_SERA']]\n",
    "        # Reorder the columns\n",
    "        file_df = file_df[desired_columns_order]\n",
    "\n",
    "        samples_list.append(file_df)  # Append each DataFrame to the list\n",
    "        samples = pd.concat(samples_list, ignore_index=True)  # Concatenate the list of DataFrames into one\n",
    "\n",
    "        samples.to_csv('./'+str(keys)+'_df.tsv', index = False,  sep='\\t')\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of TSV file names\n",
    "#filtered_files = sorted(Path(savepath).rglob('*_gene_df.tsv')) #Gene Filtration\n",
    "filtered_files = sorted(Path(savepath).rglob('*_df.tsv')) #Standard Filtration\n",
    "print(filtered_files)\n",
    "# Read TSV files into a list of DataFrames\n",
    "filt_df = [pd.read_csv(ff, sep='\\t') for ff in filtered_files]\n",
    "\n",
    "# Append DataFrames vertically\n",
    "appended_df = pd.concat(filt_df, ignore_index=True)\n",
    "\n",
    "# Write the appended DataFrame to a new TSV file\n",
    "appended_df.to_csv(savepath+'\\\\'+'appended_file.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appended File converted to Excel file\n",
    "\n",
    "# Create a new DataFrame for the Sample List\n",
    "df_sample_list = pd.DataFrame(columns=['Original Sample List', 'Samples after Filtration'])\n",
    "\n",
    "# Read values from text file for the first column\n",
    "with open(sample_list, 'r') as file: #, encoding='utf-16-le'\n",
    "    original_samples = file.read().splitlines()\n",
    "\n",
    "# Get unique values from the 'SID' column of appended_df for the second column\n",
    "filtered_samples = appended_df['SID'].unique().tolist()\n",
    "\n",
    "# Pad the shorter list with NaN values to match the length of the longer list\n",
    "max_length = max(len(original_samples), len(filtered_samples))\n",
    "original_samples += [np.nan] * (max_length - len(original_samples))\n",
    "filtered_samples += [np.nan] * (max_length - len(filtered_samples))\n",
    "\n",
    "# Create DataFrame with columns of different lengths\n",
    "df_sample_list = pd.DataFrame({'Original Sample List': original_samples, 'Samples after Filtration': filtered_samples})\n",
    "\n",
    "#Create a new Excel writer object\n",
    "with pd.ExcelWriter('output_Filtered.xlsx') as writer:\n",
    "    # Write the TSV data to the first sheet (Filtered File)\n",
    "    appended_df.to_excel(writer, sheet_name='Filtered File', index=False)\n",
    "\n",
    "    # Write the Sample List DataFrame to the second sheet\n",
    "    df_sample_list.to_excel(writer, sheet_name='Sample List', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
