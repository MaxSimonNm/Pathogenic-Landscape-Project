{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Anno Merged File Batch Processor\n",
    "\n",
    "Created on Mon Dec 5 11:37:53 2022\n",
    "\n",
    "@author: nilesh@4basecare.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# To ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting: 100%|██████████| 2553/2553 [00:15<00:00, 165.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For converting csv file to tsv file\n",
    "# Checking indir, if the folder exists and contains \"_merged_output.csv\"\n",
    "folder_path = r\"E:\\indigene\"\n",
    "\n",
    "def folder_contains_csv(folder_path):\n",
    "    if not os.path.exists(folder_path) or not os.path.isdir(folder_path):\n",
    "        print(f\"The provided path '{folder_path}' is not a valid folder.\")\n",
    "        return False\n",
    "\n",
    "    # List all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Check if any of the files have a \".csv\" extension\n",
    "    for file in files:\n",
    "        if file.lower().endswith('_filtered.csv'):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# Specify the input directory containing CSV files and the output directory for TSV files\n",
    "input_directory = folder_path\n",
    "output_directory = r\"E:\\indigene\\tsv\"\n",
    "\n",
    "# Ensure the output directory exists, create it if necessary\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Get a list of CSV files in the input directory\n",
    "tsv_files = [filename for filename in os.listdir(input_directory) if filename.endswith('.csv')]\n",
    "\n",
    "# Initialize the progress bar\n",
    "with tqdm(total=len(tsv_files), desc=\"Converting\") as pbar:\n",
    "    # Iterate through all CSV files in the input directory\n",
    "    for filename in tsv_files:\n",
    "        # Construct the full paths for input and output files\n",
    "        input_csv_file = os.path.join(input_directory, filename)\n",
    "        output_tsv_file = os.path.join(output_directory, filename[:-4] + '.tsv')\n",
    "\n",
    "        # Open the CSV file for reading and the TSV file for writing\n",
    "        with open(input_csv_file, 'r') as csv_file, open(output_tsv_file, 'w', newline='') as tsv_file:\n",
    "            # Create a CSV reader and TSV writer\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "\n",
    "            # Iterate through each row in the CSV file and write it to the TSV file\n",
    "            for row in csv_reader:\n",
    "                tsv_writer.writerow(row)\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# Finished converting all CSV files to TSV files in the directory\n",
    "print(\"Conversion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O and Chunk Settings\n",
    "\n",
    "## Folder containing *TSV* MultiAnno Merged Output of Sample's VCF\n",
    "dirpath = r\"E:\\indigene\\tsv\"\n",
    "\n",
    "## Folder Path for Saving the Chunks\n",
    "savepath = r\"E:\\indigene\\combine\"\n",
    "\n",
    "## Sample List\n",
    "sample_list = r\"E:\\indigene\\tsv\\indigene_samp.txt\"\n",
    "\n",
    "# Path for Genes file for Gene based filtration\n",
    "# genes = pd.read_csv(\"/home/bioinfo/Nilesh/HRRdb_Samples/Scripts/HRR_genes.txt\", sep='/t')\n",
    "#genes = [\"MSH2\",\"MSH3\",\"MSH5\",\"MSH6\",\"MLH1\",\"MLH2\",\"MLH3\",\"MLH4\"]\n",
    "#genes_series = pd.Series(genes, name=\"MMR Genes\")\n",
    "\n",
    "# Set Number of files per chunk.\n",
    "## If total files is 5, and chunk size is 3,\n",
    "## then 2 folders will be made,\n",
    "## 1st folder will have 3 files, 2nd one will have 2 files\n",
    "chunk_size = 700\n",
    "\n",
    "# Sample File listing\n",
    "tsv_files = [filename for filename in os.listdir(dirpath) if filename.endswith('_filtered.tsv')]\n",
    "\n",
    "# Chunk List creation based on  Chunk Size\n",
    "chunked_list = [tsv_files[i:i+chunk_size] for i in range(0, len(tsv_files), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=dict(enumerate(chunked_list))  #Nested List converted to Dictionary\n",
    "\n",
    "dt_list = [[k,v] for k, values in dt.items() for v in values]\n",
    "\n",
    "# dt_list = []                     #For Loop Expansion of above List Comprehension for Understanding\n",
    "# for keys, values in dt.items():  #for making file list per chunk used later for copying\n",
    "#   for value in values:\n",
    "#       dt_list.append([keys, value])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_index, file_name in dt_list:\n",
    "    source_path = f\"{dirpath}\\\\{file_name}\"\n",
    "    destination_path = f\"{savepath}\\\\Folder{folder_index}\"\n",
    "\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "    # Copy the file\n",
    "    shutil.copy(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:  25%|██▌       | 1/4 [00:36<01:49, 36.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:  50%|█████     | 2/4 [01:01<00:58, 29.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:  75%|███████▌  | 3/4 [01:31<00:29, 29.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 100%|██████████| 4/4 [01:47<00:00, 26.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# Standard Filtration\n",
    "for keys in tqdm(dt, desc=\"Processing folders\"):\n",
    "    folder_path = f\"{savepath}\\\\Folder{keys}\"\n",
    "    print(\"####\")\n",
    "\n",
    "    os.chdir(folder_path)\n",
    "    key_files = sorted(Path('./').glob('*.tsv'))\n",
    "    #print(key_files)\n",
    "\n",
    "\n",
    "    samples_list = []  # Create a list to store DataFrames\n",
    "\n",
    "    for file in tqdm(key_files, desc=f\"Processing files in Folder{keys}\", leave=False):\n",
    "        samp_name = file.name.replace(\"_filtered.tsv\", \"\")\n",
    "        sel_cols = ['CHROM_x', 'POS_x', 'End_x', 'REF_x', 'ALT_x', 'Ref.Gene', 'Func.ensGene', 'ExonicFunc.ensGene', 'AAChange.ensGene', 'Interpro_domain', 'avsnp150', 'CLNDN', 'CLNDISDB', 'clinvar: Clinvar ']  # f'{samp_name}:GT'\n",
    "        file_df = pd.read_csv(file, sep='\\t', usecols= sel_cols, low_memory=False)\n",
    "        file_df['SID'] = samp_name\n",
    "        file_df['Chr_SERA'] = file_df['CHROM_x'].astype(str) + ' | ' + file_df['POS_x'].astype(str) + ' | ' + file_df['End_x'].astype(str) + ' | ' + file_df['REF_x'].astype(str) + ' | ' + file_df['ALT_x'].astype(str)\n",
    "\n",
    "        # List of columns to be removed\n",
    "        static_columns_to_remove = ['CHROM_x', 'POS_x', 'End_x', 'REF_x', 'ALT_x']\n",
    "\n",
    "        # Drop the specified columns\n",
    "        file_df.drop(columns= static_columns_to_remove, inplace=True)\n",
    "\n",
    "        # Specify the desired order of columns\n",
    "        desired_columns_order = ['SID', 'Chr_SERA'] + [col for col in file_df.columns if col not in ['SID', 'Chr_SERA']]\n",
    "        # Reorder the columns\n",
    "        file_df = file_df[desired_columns_order]\n",
    "\n",
    "        samples_list.append(file_df)  # Append each DataFrame to the list\n",
    "        samples = pd.concat(samples_list, ignore_index=True)  # Concatenate the list of DataFrames into one\n",
    "\n",
    "        samples.to_csv('./'+str(keys)+'_df.tsv', index = False,  sep='\\t')\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('E:/indigene/combine/Folder0/0_df.tsv'), WindowsPath('E:/indigene/combine/Folder1/1_df.tsv'), WindowsPath('E:/indigene/combine/Folder2/2_df.tsv'), WindowsPath('E:/indigene/combine/Folder3/3_df.tsv')]\n"
     ]
    }
   ],
   "source": [
    "# List of TSV file names\n",
    "#filtered_files = sorted(Path(savepath).rglob('*_gene_df.tsv')) #Gene Filtration\n",
    "filtered_files = sorted(Path(savepath).rglob('*_df.tsv')) #Standard Filtration\n",
    "print(filtered_files)\n",
    "# Read TSV files into a list of DataFrames\n",
    "filt_df = [pd.read_csv(ff, sep='\\t') for ff in filtered_files]\n",
    "\n",
    "# Append DataFrames vertically\n",
    "appended_df = pd.concat(filt_df, ignore_index=True)\n",
    "\n",
    "# Write the appended DataFrame to a new TSV file\n",
    "appended_df.to_csv(savepath+'\\\\'+'appended_file.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appended File converted to Excel file\n",
    "\n",
    "# Create a new DataFrame for the Sample List\n",
    "df_sample_list = pd.DataFrame(columns=['Original Sample List', 'Samples after Filtration'])\n",
    "\n",
    "# Read values from text file for the first column\n",
    "with open(sample_list, 'r') as file: #, encoding='utf-16-le'\n",
    "    original_samples = file.read().splitlines()\n",
    "\n",
    "# Get unique values from the 'SID' column of appended_df for the second column\n",
    "filtered_samples = appended_df['SID'].unique().tolist()\n",
    "\n",
    "# Pad the shorter list with NaN values to match the length of the longer list\n",
    "max_length = max(len(original_samples), len(filtered_samples))\n",
    "original_samples += [np.nan] * (max_length - len(original_samples))\n",
    "filtered_samples += [np.nan] * (max_length - len(filtered_samples))\n",
    "\n",
    "# Create DataFrame with columns of different lengths\n",
    "df_sample_list = pd.DataFrame({'Original Sample List': original_samples, 'Samples after Filtration': filtered_samples})\n",
    "\n",
    "#Create a new Excel writer object\n",
    "with pd.ExcelWriter('output_Filtered.xlsx') as writer:\n",
    "    # Write the TSV data to the first sheet (Filtered File)\n",
    "    appended_df.to_excel(writer, sheet_name='Filtered File', index=False)\n",
    "\n",
    "    # Write the Sample List DataFrame to the second sheet\n",
    "    df_sample_list.to_excel(writer, sheet_name='Sample List', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
