{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Anno Merged File Batch Processor\n",
    "\n",
    "Created on Mon Dec 5 11:37:53 2022\n",
    "\n",
    "@author: nilesh@4basecare.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# To ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For converting csv file to tsv file\n",
    "# Checking indir, if the folder exists and contains \"_merged_output.csv\"\n",
    "folder_path = r\"H:\\Shared drives\\VCF & Multianno Backup\\MAMs\\MAM_17May24\"\n",
    "\n",
    "def folder_contains_csv(folder_path):\n",
    "    if not os.path.exists(folder_path) or not os.path.isdir(folder_path):\n",
    "        print(f\"The provided path '{folder_path}' is not a valid folder.\")\n",
    "        return False\n",
    "\n",
    "    # List all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Check if any of the files have a \".csv\" extension\n",
    "    for file in files:\n",
    "        if file.lower().endswith('_merged_output.csv'):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# Specify the input directory containing CSV files and the output directory for TSV files\n",
    "input_directory = folder_path\n",
    "output_directory = r\"F:\\4bc\\MSI Project\\MSI S6\\mam_tsv\"\n",
    "\n",
    "# Ensure the output directory exists, create it if necessary\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Get a list of CSV files in the input directory\n",
    "csv_files = [filename for filename in os.listdir(input_directory) if filename.endswith('.csv')]\n",
    "\n",
    "# Initialize the progress bar\n",
    "with tqdm(total=len(csv_files), desc=\"Converting\") as pbar:\n",
    "    # Iterate through all CSV files in the input directory\n",
    "    for filename in csv_files:\n",
    "        # Construct the full paths for input and output files\n",
    "        input_csv_file = os.path.join(input_directory, filename)\n",
    "        output_tsv_file = os.path.join(output_directory, filename[:-4] + '.tsv')\n",
    "\n",
    "        # Open the CSV file for reading and the TSV file for writing\n",
    "        with open(input_csv_file, 'r') as csv_file, open(output_tsv_file, 'w', newline='') as tsv_file:\n",
    "            # Create a CSV reader and TSV writer\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "\n",
    "            # Iterate through each row in the CSV file and write it to the TSV file\n",
    "            for row in csv_reader:\n",
    "                tsv_writer.writerow(row)\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# Finished converting all CSV files to TSV files in the directory\n",
    "print(\"Conversion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O and Chunk Settings\n",
    "\n",
    "## Folder containing MultiAnno Merged Output of Sample's VCF\n",
    "dirpath = r\"H:\\Shared drives\\Amol_GBC\\GBC_Project\\DNA\\GERMLINE\\Gmam\\mam_tsv\"\n",
    "\n",
    "## Folder Path for Saving the Chunks\n",
    "savepath = r\"H:\\Shared drives\\Amol_GBC\\GBC_Project\\DNA\\GERMLINE\\Gmam\\Gmam_MAF_0.1_Exonic-Splicing_yes_syno_Filtered\"\n",
    "\n",
    "## Sample List\n",
    "sample_list = r\"H:\\Shared drives\\Amol_GBC\\GBC_Project\\DNA\\GERMLINE\\Gmam\\Gmam_samples.txt\"\n",
    "\n",
    "# Path for Genes file for Gene based filtration\n",
    "# genes = pd.read_csv(\"/home/bioinfo/Nilesh/HRRdb_Samples/Scripts/HRR_genes.txt\", sep='/t')\n",
    "#genes = [\"MSH2\",\"MSH3\",\"MSH5\",\"MSH6\",\"MLH1\",\"MLH2\",\"MLH3\",\"MLH4\"]\n",
    "#genes_series = pd.Series(genes, name=\"MMR Genes\")\n",
    "\n",
    "# Set Number of files per chunk.\n",
    "## If total files is 5, and chunk size is 3,\n",
    "## then 2 folders will be made,\n",
    "## 1st folder will have 3 files, 2nd one will have 2 files\n",
    "chunk_size = 650\n",
    "\n",
    "# Sample File listing\n",
    "csv_files = [filename for filename in os.listdir(dirpath) if filename.endswith('_merged_output.tsv')]\n",
    "\n",
    "# Chunk List creation based on  Chunk Size\n",
    "chunked_list = [csv_files[i:i+chunk_size] for i in range(0, len(csv_files), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=dict(enumerate(chunked_list))  #Nested List converted to Dictionary\n",
    "\n",
    "dt_list = [[k,v] for k, values in dt.items() for v in values]\n",
    "\n",
    "# dt_list = []                     #For Loop Expansion of above List Comprehension for Understanding\n",
    "# for keys, values in dt.items():  #for making file list per chunk used later for copying\n",
    "#   for value in values:\n",
    "#       dt_list.append([keys, value])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_index, file_name in dt_list:\n",
    "    source_path = f\"{dirpath}\\\\{file_name}\"\n",
    "    destination_path = f\"{savepath}\\\\Folder{folder_index}\"\n",
    "\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "    # Copy the file\n",
    "    shutil.copy(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Filtration\n",
    "for keys in tqdm(dt, desc=\"Processing folders\"):\n",
    "    folder_path = f\"{savepath}\\\\Folder{keys}\"\n",
    "    print(\"####\")\n",
    "\n",
    "    os.chdir(folder_path)\n",
    "    key_files = sorted(Path('./').glob('*.tsv'))\n",
    "    #print(key_files)\n",
    "\n",
    "\n",
    "    samples_list = []  # Create a list to store DataFrames\n",
    "\n",
    "    for file in tqdm(key_files, desc=f\"Processing files in Folder{keys}\", leave=False):\n",
    "        samp_name = file.name.replace(\"_merged_output.tsv\", \"\")\n",
    "        sel_cols = ['CHROM_x', 'POS_x', 'End_x', 'REF_x', 'ALT_x', 'Ref.Gene', 'Func.ensGene', 'ExonicFunc.ensGene', 'AAChange.ensGene', 'Interpro_domain', 'avsnp150', 'CLNDN', 'CLNDISDB', 'clinvar: Clinvar ', 'InterVar_automated', 'intervar_inhouse', ' CancerVar: CancerVar and Evidence ', 'OMIM', 'Pathway', 'Therap_list', 'Diag_list', 'Prog_list', 'esp6500siv2_all', 'ExAC_ALL', 'ExAC_SAS', 'AF', 'AF_sas', '1000g2015aug_all', '1000g2015aug_SAS', f'{samp_name}:AF', f'{samp_name}:DP', 'Mutant_allelic_burden_%', 'ensemble_value', 'Ref_Depth', 'Mutant_Depth', 'FILTER']  # f'{samp_name}:GT'\n",
    "        file_df = pd.read_csv(file, sep='\\t', usecols= sel_cols, low_memory=False)\n",
    "        file_df['SID'] = samp_name\n",
    "        file_df['Chr_SERA'] = file_df['CHROM_x'].astype(str) + ' | ' + file_df['POS_x'].astype(str) + ' | ' + file_df['End_x'].astype(str) + ' | ' + file_df['REF_x'].astype(str) + ' | ' + file_df['ALT_x'].astype(str)\n",
    "\n",
    "\n",
    "        # Dynamic column name based on the filename\n",
    "        dyn_AF = f'{samp_name}:AF'\n",
    "        dyn_DP = f'{samp_name}:DP'\n",
    "        dyn_GT = f'{samp_name}:GT'\n",
    "\n",
    "        # Create a new column named 'samp_AF' with values from the dynamic column\n",
    "        file_df['samp_AF'] = file_df[dyn_AF]\n",
    "        file_df['samp_DP'] = file_df[dyn_DP]\n",
    "        #file_df['samp_GT'] = file_df[dyn_GT]\n",
    "\n",
    "\n",
    "        # List of columns to be removed\n",
    "        dynamic_columns_to_remove = [dyn_AF, dyn_DP] #dyn_GT\n",
    "        static_columns_to_remove = ['CHROM_x', 'POS_x', 'End_x', 'REF_x', 'ALT_x']\n",
    "\n",
    "\n",
    "        # Drop the specified columns\n",
    "        file_df.drop(columns= dynamic_columns_to_remove, inplace=True)\n",
    "        file_df.drop(columns= static_columns_to_remove, inplace=True)\n",
    "\n",
    "\n",
    "        # Specify the desired order of columns\n",
    "        desired_columns_order = ['SID', 'Chr_SERA'] + [col for col in file_df.columns if col not in ['SID', 'Chr_SERA']]\n",
    "        # Reorder the columns\n",
    "        file_df = file_df[desired_columns_order]\n",
    "\n",
    "\n",
    "        samples_list.append(file_df)  # Append each DataFrame to the list\n",
    "        samples = pd.concat(samples_list, ignore_index=True)  # Concatenate the list of DataFrames into one\n",
    "\n",
    "        ## Filter Section ##\n",
    "        \"\"\"\n",
    "        Filtration of Certain Values:\n",
    "        FILTER = Only PASS\n",
    "        Func.ensGene = only Exonic, Splicing and Exonic;Splicing\n",
    "        ExonicFunc.ensGene = remove synonymous SNV\n",
    "        clinvar: Clinvar = Remove Unknown, Uncertain Significance, Benign\n",
    "        Intervar_auto = Only Pathogenic\n",
    "        Population Frequency Data = Only lesser than or equal to 0.01\n",
    "        Ref_Depth = Only greather than 2\n",
    "\n",
    "        # Gene Filtration if needed\n",
    "        \"\"\"\n",
    "        # filter for rows containing \"PASS\" in FILTER coloumn\n",
    "        # samples_filtered = samples[samples['FILTER'].str.contains(\"pass\", na=False, case=False)]\n",
    "\n",
    "        # filter for rows containing \"exonic\" and \"splicing\", but not \"ncR\"\n",
    "        samples_filtered = samples[samples[\"Func.ensGene\"].str.contains(\"exonic|splicing|exonic;splicing\") & ~samples[\"Func.ensGene\"].str.contains(\"ncR\")]\n",
    "\n",
    "        # filter out rows containing the string \"synonymous SNV\"\n",
    "        # samples_filtered = samples_filtered[samples_filtered['ExonicFunc.ensGene'] != 'synonymous SNV']\n",
    "\n",
    "        # filter out rows containing unknown, uncertain significance and benign in clinvar: Clinvar column\n",
    "        # samples_filtered = samples_filtered[(~samples_filtered['clinvar: Clinvar '].str.contains(\"unk\", na=False, case=False)) & (~samples_filtered['clinvar: Clinvar '].str.contains(\"uncertain\", na=False, case=False)) & (~samples_filtered['clinvar: Clinvar '].str.contains(\"benign\", na=False, case=False))]\n",
    "\n",
    "        # filter for rows containing only Pathogenic in Intervar auto column\n",
    "        # samples_filtered = samples_filtered[samples_filtered['InterVar_automated'].str.contains(\"pathogenic\", na=False, case=False)]\n",
    "\n",
    "        # filtering out pop freq columns for values less than or equal to 0.1\n",
    "        popfreqs=['esp6500siv2_all','ExAC_ALL','ExAC_SAS','AF','AF_sas','1000g2015aug_all','1000g2015aug_SAS']\n",
    "\n",
    "        for col in popfreqs:\n",
    "                samples_filtered[col] = samples_filtered[col].replace('.',0).fillna(0)\n",
    "                samples_filtered[col] = samples_filtered[col].astype(float).round(4)\n",
    "                samples_filtered=samples_filtered[samples_filtered[col]<= 0.1]\n",
    "\n",
    "        # filter for rows greater than 2 in Ref depth\n",
    "        # values_to_exclude = [\"2\", \"1\", \"0\"]\n",
    "        # file_df = file_df[~file_df['Ref_Depth'].isin(values_to_exclude)]\n",
    "\n",
    "        samples_filtered.to_csv('./'+str(keys)+'_df.tsv', index = False,  sep='\\t')\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of TSV file names\n",
    "#filtered_files = sorted(Path(savepath).rglob('*_gene_df.tsv')) #Gene Filtration\n",
    "filtered_files = sorted(Path(savepath).rglob('*_df.tsv')) #Standard Filtration\n",
    "print(filtered_files)\n",
    "# Read TSV files into a list of DataFrames\n",
    "filt_df = [pd.read_csv(ff, sep='\\t') for ff in filtered_files]\n",
    "\n",
    "# Append DataFrames vertically\n",
    "appended_df = pd.concat(filt_df, ignore_index=True)\n",
    "\n",
    "# Write the appended DataFrame to a new TSV file\n",
    "appended_df.to_csv(savepath+'\\\\'+'appended_file.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appended File converted to Excel file\n",
    "\n",
    "# Create a new DataFrame for the Sample List\n",
    "df_sample_list = pd.DataFrame(columns=['Original Sample List', 'Samples after Filtration'])\n",
    "\n",
    "# Read values from text file for the first column\n",
    "with open(sample_list, 'r') as file: #, encoding='utf-16-le'\n",
    "    original_samples = file.read().splitlines()\n",
    "\n",
    "# Get unique values from the 'SID' column of appended_df for the second column\n",
    "filtered_samples = appended_df['SID'].unique().tolist()\n",
    "\n",
    "# Pad the shorter list with NaN values to match the length of the longer list\n",
    "max_length = max(len(original_samples), len(filtered_samples))\n",
    "original_samples += [np.nan] * (max_length - len(original_samples))\n",
    "filtered_samples += [np.nan] * (max_length - len(filtered_samples))\n",
    "\n",
    "# Create DataFrame with columns of different lengths\n",
    "df_sample_list = pd.DataFrame({'Original Sample List': original_samples, 'Samples after Filtration': filtered_samples})\n",
    "\n",
    "#Create a new Excel writer object\n",
    "with pd.ExcelWriter('output_Filtered.xlsx') as writer:\n",
    "    # Write the TSV data to the first sheet (Filtered File)\n",
    "    appended_df.to_excel(writer, sheet_name='Filtered File', index=False)\n",
    "\n",
    "    # Write the Sample List DataFrame to the second sheet\n",
    "    df_sample_list.to_excel(writer, sheet_name='Sample List', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
